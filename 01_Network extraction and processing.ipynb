{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-06T01:07:22.640095Z",
     "start_time": "2021-09-06T01:07:21.504560Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import osmnx as ox\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import shapely\n",
    "import pickle\n",
    "\n",
    "import shutil\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import warnings; warnings.simplefilter('ignore')\n",
    "\n",
    "pd.options.display.max_columns = None\n",
    "pd.options.display.max_rows = 30\n",
    "np.set_printoptions(threshold=sys.maxsize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract road network dataset from OpenStreetMap (OSM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define coordinate reference system: WGS84 - World Geodetic System 1984 used in GPS\n",
    "crs_lonlat = {'init': 'epsg:' + str(4326)}\n",
    "# Directory where the boundaries for networks (in shapefile format) are stored\n",
    "shapefile_bnd_dir = '/Users/yiyi/Library/CloudStorage/OneDrive-GeorgiaInstituteofTechnology(2)/Research/Global road network resilience/01_data/4229_bnd_convex'\n",
    "\n",
    "# Extract OSM network using the boundary shapefiles\n",
    "counter = 0\n",
    "for i, file in enumerate(os.listdir(shapefile_bnd_dir)):\n",
    "    if file[-4:] == '.shp':\n",
    "        bnd = gpd.read_file(shapefile_bnd_dir+file).to_crs(crs_lonlat)\n",
    "        net_id = bnd['Id'][0]\n",
    "        try:                 \n",
    "            G = ox.graph_from_polygon(bnd.geometry[0], network_type='drive_service')\n",
    "            counter += 1\n",
    "\n",
    "            with open('/05_WB/CityResilience/1_processed/raw_osmgraphs_drive_service_conv/g_raw_drive_conv_' + str(net_id) + '.pk', 'wb') as handle:\n",
    "                pickle.dump(G, handle, protocol=2)\n",
    "        except:\n",
    "            print(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Summarize network/graph properties:**\n",
    "- Number of nodes\n",
    "- Number of edges\n",
    "- Tertiary road lengths\n",
    "- Residential road lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate summary dataframe\n",
    "df = pd.DataFrame(columns=['net_id', 'nodes', 'edges','tot_length','tertiary_length','residential_length'], index=range(4194))\n",
    "counter = 0\n",
    "\n",
    "# Calculate the lengths for tertiary as well as residential road types\n",
    "for file in os.listdir(network_dir):\n",
    "    G = pickle.load(open(network_dir + file, 'rb'))\n",
    "    tertiary_length = []\n",
    "    residential_length = []\n",
    "    net_id = file[6:][:-3]\n",
    "    # Populate dataframe with network id, number of nodes, number of edges, total road length\n",
    "    df.at[counter, 'net_id'] = net_id\n",
    "    df.at[counter, 'nodes'] = G.number_of_nodes()\n",
    "    df.at[counter, 'edges'] = G.number_of_edges()\n",
    "    df.at[counter, 'tot_length'] = G.size(weight=\"length\")\n",
    "    # Examine the \"highway\" attribute of the OSM networks/graphs for road lengths\n",
    "    for i,j,data in G.edges.data():\n",
    "        if data['highway'] == 'tertiary':\n",
    "            tertiary_length.append(data['length'])\n",
    "        elif data['highway'] == 'tertiary_link':\n",
    "            tertiary_length.append(data['length'])\n",
    "        elif data['highway'] == 'residential':\n",
    "            residential_length.append(data['length'])\n",
    "    # Populate dataframe with tertiary road length and residential road length\n",
    "    df.at[counter, 'tertiary_length'] = sum(tertiary_length)\n",
    "    df.at[counter, 'residential_length'] = sum(residential_length)\n",
    "    counter += 1\n",
    "df.to_csv('/05_WB/CityResilience/1_processed/graphs_drive_service_tertiary_road_lengths_4194.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examine different road types (\"highway\") in the OSM road network dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to remove list nestings\n",
    "def reemovNestings(l): \n",
    "    for i in l: \n",
    "        if type(i) == list: \n",
    "            reemovNestings(i)\n",
    "        else: \n",
    "            output.append(i)\n",
    "\n",
    "# Initiate dataframe to store road types\n",
    "df = pd.DataFrame(columns=['net_id', 'net_types'], index=range(4190))\n",
    "counter = 0\n",
    "net_type_lst = [] # Initiate list of network types\n",
    "\n",
    "# For each network, summarize road type\n",
    "for file in os.listdir(network_dir):\n",
    "    G = pickle.load(open(network_dir + file, 'rb'))\n",
    "    net_id = file[6:][:-3]\n",
    "    df.at[counter, 'net_id'] = net_id\n",
    "    highway_class = nx.get_edge_attributes(G,'highway')\n",
    "    output = [] \n",
    "    reemovNestings(highway_class.values())\n",
    "    df.at[counter, 'net_types'] = list(set(output))\n",
    "    net_type_lst.append(list(set(output)))\n",
    "    counter += 1\n",
    "    \n",
    "output = []\n",
    "reemovNestings(net_type_lst)\n",
    "\n",
    "road_type_count_df = pd.DataFrame(columns=['highway_class', 'count'], index=range(171))\n",
    "counter = 0\n",
    "\n",
    "for item in list(set(output)):\n",
    "    road_type_count_df.at[counter, 'highway_class'] = item\n",
    "    road_type_count_df.at[counter, 'count'] = output.count(item)\n",
    "    counter += 1\n",
    "\n",
    "road_type_count_df.to_csv('/05_WB/CityResilience/1_processed/Drive_service_conv_highway_class_count.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove residential roads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize directory to store processed networks/graphs\n",
    "network_dir = 'L:/yiyi/graphs_cov_no_res2/'\n",
    "\n",
    "# Remove residential roads\n",
    "for file in tqdm(os.listdir(network_dir)):\n",
    "    net_id = file[15:][:-3]\n",
    "    G = pickle.load(open(network_dir + file, 'rb'))\n",
    "    G2 = G.copy()\n",
    "    for i,j,data in G.edges.data():\n",
    "        if data['highway'] == 'residential':\n",
    "            G2.remove_edge(i, j)\n",
    "        elif  'residential' in data['highway']:\n",
    "            G2.remove_edge(i, j)\n",
    "    # Remove isolated nodes as a result of the edge removal\n",
    "    G2.remove_nodes_from(list(nx.isolates(G2)))\n",
    "    with open('L:yiyi/graphs_cov_no_res3/graph_cov_no_r_' + str(net_id) + '.pk', 'wb') as handle:\n",
    "                pickle.dump(G2, handle, protocol=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare original convex hull area with graph convex hull area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_convex_hull_shp_dir = 'L:/yiyi/4190_graph_cov/'\n",
    "graph_conv_sqkm_df = pd.DataFrame(columns=['net_id', 'graph_conv_sqkm'], index=range(4188))\n",
    "counter = 0\n",
    "\n",
    "for file in tqdm(os.listdir(graph_convex_hull_shp_dir)):\n",
    "    if file[-3:] == 'shp':\n",
    "        net_id = int(file[2:][:-14])\n",
    "        data = gpd.read_file(graph_convex_hull_shp_dir+file)\n",
    "        data_meters = data.to_crs({'init': 'epsg:6933'})\n",
    "        sqkm = data_meters['geometry'].area/ 10**6\n",
    "        \n",
    "        graph_conv_sqkm_df.at[counter, 'net_id'] = net_id\n",
    "        graph_conv_sqkm_df.at[counter, 'graph_conv_sqkm'] = sqkm\n",
    "        \n",
    "        counter += 1\n",
    "        \n",
    "graph_conv_sqkm_df['graph_conv_sqkm_simplified'] = graph_conv_sqkm_df.apply(lambda row: float(row['graph_conv_sqkm']), axis=1)\n",
    "graph_conv_sqkm_df[['net_id', 'graph_conv_sqkm_simplified']].to_csv('L:/yiyi/graph_4188_conv_sqkm.csv')\n",
    "\n",
    "original_convex_hulls = gpd.read_file('H:/05_WB/CityResilience/1_processed/final_bnd/final_bnd/urcls_4190_conv_prop.shp')\n",
    "original_convex_hulls_meters = original_convex_hulls.to_crs({'init': 'epsg:6933'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ori_graph_conv_merge = pd.merge(original_convex_hulls_meters[['net_id', 'python_sqkm']], graph_conv_sqkm_df[['net_id', 'graph_conv_sqkm_simplified']],\n",
    "                               left_on = 'net_id', right_on = 'net_id', how = 'inner')\n",
    "ori_graph_conv_merge['missing_percentage'] = ori_graph_conv_merge.apply(lambda row:\n",
    "                                                                        (row['python_sqkm'] - row['graph_conv_sqkm_simplified'])*100/row['python_sqkm'],\n",
    "                                                                        axis = 1)\n",
    "num_graph_df = pd.DataFrame(columns=['missing_percentage', 'count_of_4188'])\n",
    "for i in range(100):\n",
    "    missing_percentage = i+1\n",
    "    count = ori_graph_conv_merge[ori_graph_conv_merge['missing_percentage']<missing_percentage].shape[0]\n",
    "    num_graph_df.at[i, 'missing_percentage'] = missing_percentage\n",
    "    num_graph_df.at[i, 'count_of_4188'] = count\n",
    "\n",
    "num_graph_df.to_csv('L:/yiyi/missing_percentage_count_of_4188.csv')\n",
    "\n",
    "# KNEE POINT / ELBOW POINT\n",
    "y = list(num_graph_df['count_of_4188'].values)\n",
    "x = range(1, len(y)+1)\n",
    "\n",
    "kn = KneeLocator(x, y, curve='concave', direction='increasing')\n",
    "print(kn.knee)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarize the number of nodes, edges and length of roads for 4190 networks\n",
    "- nodes\n",
    "- edges\n",
    "- tot_length\n",
    "- primary road length\n",
    "- secondary road length\n",
    "- tertiary road length\n",
    "- motorway length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns=['net_id', 'nodes', 'edges','tot_length',\n",
    "                           'primary_length',\n",
    "                           'secondary_length',\n",
    "                           'tertiary_length', \n",
    "                           'motorway_length', \n",
    "                           'unclassified_length',\n",
    "                           'service_length',\n",
    "                           'trunk_length', \n",
    "                           'living_street_length',\n",
    "                           'road_length',\n",
    "                           'residential_length' ], index=range(4190))\n",
    "counter = 0\n",
    "\n",
    "for file in os.listdir(network_dir):\n",
    "    G = pickle.load(open(network_dir + file, 'rb'))\n",
    "    \n",
    "    primary_length = []\n",
    "    secondary_length = []\n",
    "    tertiary_length = []\n",
    "    motorway_length = []\n",
    "    unclassified_length = []\n",
    "    service_length = []\n",
    "    trunk_length = []\n",
    "    living_street_length = []\n",
    "    road_length = []\n",
    "    residential_length = []\n",
    "    \n",
    "    net_id = file[15:][:-3]\n",
    "    df.at[counter, 'net_id'] = net_id\n",
    "    df.at[counter, 'nodes'] = G.number_of_nodes()\n",
    "    df.at[counter, 'edges'] = G.number_of_edges()\n",
    "    df.at[counter, 'tot_length'] = G.size(weight=\"length\")\n",
    "    for i,j,data in G.edges.data():\n",
    "        if data['highway'] == 'primary' or data['highway'] == 'primary_link':\n",
    "            primary_length.append(data['length'])\n",
    "            \n",
    "        elif data['highway'] == 'secondary' or data['highway'] == 'secondary_link' :\n",
    "            secondary_length.append(data['length'])\n",
    "            \n",
    "        elif data['highway'] == 'tertiary' or data['highway'] == 'tertiary_link':\n",
    "            tertiary_length.append(data['length'])\n",
    "        \n",
    "        elif data['highway'] == 'motorway' or data['highway'] == 'motorway_link':\n",
    "            motorway_length.append(data['length'])\n",
    "            \n",
    "        elif data['highway'] == 'unclassified':\n",
    "            unclassified_length.append(data['length'])\n",
    "        \n",
    "        elif data['highway'] == 'service':\n",
    "            service_length.append(data['length'])\n",
    "        \n",
    "        elif data['highway'] == 'trunk' or data['highway'] == 'trunk_link':\n",
    "            trunk_length.append(data['length'])\n",
    "            \n",
    "        elif data['highway'] == 'living_street':\n",
    "            living_street_length.append(data['length'])\n",
    "        \n",
    "        elif data['highway'] == 'road':\n",
    "            road_length.append(data['length'])\n",
    "        \n",
    "        elif data['highway'] == 'residential':\n",
    "            residential_length.append(data['length'])\n",
    "            \n",
    "    df.at[counter, 'primary_length'] = sum(primary_length) \n",
    "    df.at[counter, 'secondary_length'] = sum(secondary_length)\n",
    "    df.at[counter, 'tertiary_length'] = sum(tertiary_length)\n",
    "    df.at[counter, 'motorway_length'] = sum(motorway_length)\n",
    "    df.at[counter, 'unclassified_length'] = sum(unclassified_length)\n",
    "    df.at[counter, 'service_length'] = sum(service_length)\n",
    "    df.at[counter, 'trunk_length'] = sum(trunk_length)\n",
    "    df.at[counter, 'living_street_length'] = sum(living_street_length)\n",
    "    df.at[counter, 'road_length'] = sum(road_length)\n",
    "    df.at[counter, 'residential_length'] = sum(residential_length)\n",
    "    counter += 1\n",
    "    \n",
    "df.to_csv('L:/yiyi/drive_service_no_res_4190_graph_properties.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "global_rd",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
