{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "import random\n",
    "import osmnx as ox\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "import shapely\n",
    "import pickle\n",
    "\n",
    "import shutil\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import warnings; warnings.simplefilter('ignore')\n",
    "\n",
    "pd.options.display.max_columns = None\n",
    "pd.options.display.max_rows = 30\n",
    "np.set_printoptions(threshold=sys.maxsize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Add flood inundation to network nodes and edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flood the nodes with 10 flood return periods\n",
    "RP_lst = [5, 10, 20, 50, 75, 100, 200, 250, 500, 1000]\n",
    "for file in tqdm(os.listdir('L:/yiyi/grth_90_graphs_drive_service_no_res/')):\n",
    "    net_id = int(file[15:][:-3])\n",
    "    G = pickle.load(open('L:/yiyi/grth_90_graphs_drive_service_no_res/'+ file, 'rb'))\n",
    "    G_flooded = G.copy()\n",
    "    # Flood the nodes with 10 scenarios\n",
    "    for rp in RP_lst:\n",
    "        G_flooded = gn.sample_raster(G_flooded, 'J:/yiyi/mosaics/FUP'+ str(rp) +'_mosaic.tif', 'FUP_'+ str(rp))\n",
    "    # Now we have flooded nodes with 10 flood depths\n",
    "    # Flood the edges too by assigning the larger nodes flood depth to edge\n",
    "    for i, j, data in G_flooded.edges.data():\n",
    "        for rp in RP_lst:\n",
    "            FUP_i = G_flooded.nodes[i]['FUP_'+str(rp)]\n",
    "            FUP_j = G_flooded.nodes[j]['FUP_'+str(rp)]\n",
    "            G_flooded[i][j][0]['FUP_'+str(rp)] = max(FUP_i, FUP_j)\n",
    "\n",
    "    with open('L:/yiyi/grth_90_graphs_10flooded/G_cov_no_r_' + str(net_id) + '_10flooded.pk', 'wb') as handle:\n",
    "                pickle.dump(G_flooded, handle, protocol=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove nodes and edges based on flood inundation level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RP_lst = [5, 10, 20, 50, 75, 100, 200, 250, 500, 1000]\n",
    "problem_graph_ids = []\n",
    "flooded_graphs_2616_dir = 'L:/yiyi/2616_flooded_graphs_drive_servce_no_res/'\n",
    "\n",
    "for rp in RP_lst:\n",
    "    print(rp)\n",
    "    for flooded_graph_file in os.listdir(flooded_graphs_2616_dir):\n",
    "        net_id = int(flooded_graph_file[11:][:-13])\n",
    "        G_flooded =  pickle.load(open(flooded_graphs_2616_dir + flooded_graph_file, 'rb'))\n",
    "        G_disrupted = G_flooded.copy()\n",
    "        \n",
    "        try:\n",
    "            # Delete nodes with flood inundation greater than 30mm or 0.3m\n",
    "            for node_id, node_data in G_flooded.nodes.data():\n",
    "                if node_data['FUP_' + str(rp)] >= 0.3:\n",
    "                    G_disrupted.remove_node(node_id)\n",
    "            # Delete edges with flood inundation greater than 30mm or 0.3m\n",
    "            for start_id, end_id, edge_data in G_flooded.edges.data():\n",
    "                if edge_data['FUP_' + str(rp)] >= 0.3:\n",
    "                    G_disrupted.remove_edge(start_id, end_id)\n",
    "        except:\n",
    "            problem_graph_ids.append(net_id)\n",
    "        # Add original/dry speed\n",
    "        try:\n",
    "            G_disrupted_speed = ox.speed.add_edge_speeds(G_disrupted)\n",
    "        except:\n",
    "            problem_graph_ids.append(net_id)\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            for start_id, end_id, edge_data in G_disrupted_speed.edges.data():\n",
    "                edge_water_depth_mm = edge_data['FUP_' + str(rp)]\n",
    "                theoretical_speed = 86.9448 - (0.5529*edge_water_depth_mm) + (0.0009*edge_water_depth_mm*edge_water_depth_mm)\n",
    "                designed_speed = edge_data['speed_kph']\n",
    "                G_disrupted_speed[start_id][end_id][0]['speed_kph'] = min(theoretical_speed, designed_speed)\n",
    "            G_disrupted_speed_time = ox.speed.add_edge_travel_times(G_disrupted_speed)\n",
    "        except:\n",
    "             problem_graph_ids.append(net_id)\n",
    "\n",
    "        with open('L:/yiyi/2616_disprupted_graphs/FUP_' + str(rp) + '/G_' + str(net_id) + '_disrupted_FUP_' + str(rp) + '.pk', 'wb') as handle:\n",
    "            pickle.dump(G_disrupted_speed_time, handle, protocol=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flood exposure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flooded_graph_dir = 'L:/yiyi/all_graph_flooded/'\n",
    "flood_10_rps = [5, 10, 20, 50, 75, 100, 200, 250, 500, 1000]\n",
    "\n",
    "df = pd.DataFrame()\n",
    "\n",
    "for file in tqdm(os.listdir(flooded_graph_dir)):\n",
    "    if file[-2:] == 'pk':\n",
    "        net_id = int(file[11:-13])\n",
    "        G = pickle.load(open('L:/yiyi/all_graph_flooded/'+ file, 'rb'))\n",
    "        graph_length_df = pd.DataFrame.from_dict(nx.get_edge_attributes(G, \"length\"), orient='index').rename(columns={0:'length'})\n",
    "        for rp in flood_10_rps:\n",
    "            graph_length_df[f'FUP_{rp}_m'] = pd.DataFrame.from_dict(nx.get_edge_attributes(G, f\"FUP_{rp}\"), orient='index')[0]\n",
    "        \n",
    "        \n",
    "        for rp in flood_10_rps:\n",
    "            ls = [net_id, rp]\n",
    "            for threshold in np.arange(0.0, 5.1, 0.1):\n",
    "                graph_len = graph_length_df[graph_length_df[f'FUP_{rp}_m'] >= threshold]['length'].sum()\n",
    "                ls.append(graph_len)\n",
    "                \n",
    "            df = df.append(pd.DataFrame([ls],\n",
    "                                        columns=['net_id','RP']+[str(round(i,1)) for i in np.arange(0.0, 5.1, 0.1)]),\n",
    "                               ignore_index = True)\n",
    "df.to_csv('../1_processed/exposure_sensitivity_raw_summary.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate disrupted networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RP_lst = [5, 10, 20, 50, 75, 100, 200, 250, 500, 1000]\n",
    "flooded_graphs_2616_dir = 'L:/yiyi/2616_flooded_graphs_drive_servce_no_res/'\n",
    "\n",
    "for rp in RP_lst:\n",
    "    print('RP=' + str(rp))\n",
    "    for flooded_graph_file in os.listdir(flooded_graphs_2616_dir):\n",
    "        net_id = int(flooded_graph_file[11:][:-13])\n",
    "        print('net_id='+ str(net_id))\n",
    "        G_flooded =  pickle.load(open(flooded_graphs_2616_dir + flooded_graph_file, 'rb'))\n",
    "        G_disrupted = G_flooded.copy()\n",
    "\n",
    "        # Delete nodes with flood inundation greater than 30mm or 0.3m\n",
    "        for node_id, node_data in G_flooded.nodes.data():\n",
    "            if node_data['FUP_' + str(rp)] >= 0.3:\n",
    "                G_disrupted.remove_node(node_id)\n",
    "\n",
    "        print('removed: '+ str(G_disrupted.number_of_nodes()) + ' from ' + str(G_flooded.number_of_nodes()))\n",
    "        # Add original/dry speed\n",
    "        try:\n",
    "            G_disrupted_speed = ox.speed.add_edge_speeds(G_disrupted)\n",
    "            print('finished adding ori speed')\n",
    "        except:\n",
    "            print('CANNOT add ori speed')\n",
    "            continue\n",
    "\n",
    "        for start_id, end_id, edge_data in G_disrupted_speed.edges.data():\n",
    "            edge_water_depth_mm = max(G_disrupted_speed.nodes[start_id]['FUP_'+str(rp)], G_disrupted_speed.nodes[end_id]['FUP_'+str(rp)])\n",
    "            theoretical_speed = 86.9448 - (0.5529*edge_water_depth_mm) + (0.0009*edge_water_depth_mm*edge_water_depth_mm)\n",
    "            designed_speed = edge_data['speed_kph']\n",
    "            G_disrupted_speed[start_id][end_id][0]['speed_kph'] = min(theoretical_speed, designed_speed)\n",
    "        try:\n",
    "            G_disrupted_speed_time = ox.speed.add_edge_travel_times(G_disrupted_speed)\n",
    "            print('finished changing speed')\n",
    "        except:\n",
    "            print('CANNOT disrupt speed and time')\n",
    "#         print('L:/yiyi/2616_disrupted_graphs/FUP_' + str(rp) + '/G_' + str(net_id) + '_disrupted_FUP_' + str(rp) + '.pk')\n",
    "        with open('L:/yiyi/2616_disrupted_graphs/FUP_' + str(rp) + '/G_' + str(net_id) + '_disrupted_FUP_' + str(rp) + '.pk', 'wb') as handle:\n",
    "            pickle.dump(G_disrupted_speed_time, handle, protocol=2)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
